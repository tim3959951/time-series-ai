# ğŸ“Š AI-Powered Time-Series Classification  

This project applies a **1D Convolutional Neural Network (1D-CNN)** to classify **time-series data** from an industrial dataset.  
The model is trained to distinguish between different operational conditions, with a focus on **anomaly detection** and **predictive maintenance**.  

---

## ğŸš€ Project Overview  

- **Dataset**: Industrial sensor readings from **IMBD AI & Big Data Competition**  
- **Model**: 1D-CNN with **Conv1D**, **MaxPooling**, and **GlobalAveragePooling**  
- **Techniques**: K-Fold Cross-Validation, Data Augmentation, Grad-CAM for interpretability  
- **Performance**: **90.9% average accuracy**, with best model achieving **91.2% accuracy**  
- **Deployment**: Future goal to optimise for **real-time monitoring & edge inference**  

---

## ğŸ“‚ Project Structure  

ğŸ“‚ time-series-ai â”‚â”€â”€ ğŸ“‚ src # Core scripts
â”‚ â”œâ”€â”€ data_loader.py # Load & preprocess time-series data
â”‚ â”œâ”€â”€ train.py # Train 1D-CNN model
â”‚ â”œâ”€â”€ evaluate.py # Evaluate model (accuracy, confusion matrix)
â”‚ â”œâ”€â”€ visualization.py # Generate Grad-CAM heatmaps & result plots
â”‚â”€â”€ ğŸ“‚ experiments # Training notebooks & performance analysis
â”‚â”€â”€ time_series_classification.ipynb # Full Jupyter Notebook for model training
â”‚â”€â”€ requirements.txt # Python dependencies
â”‚â”€â”€ README.md # Project documentation
â”‚â”€â”€ .gitignore # Ignore unnecessary files


---

## ğŸ”¬ Data Processing  

- **Source**: Time-series sensor data from **thubigdata2019training-230**  
- **Preprocessing**:  
  - Removed unit labels (Deg.F)  
  - Converted all values to `float`  
  - Normalised time-series data  
  - **Total records**: **1,745**, with **1,723 valid samples**  

---

## ğŸ— Model Architecture  

The model is a **1D-CNN** designed for time-series classification:  

```python
model = Sequential()
model.add(Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))  
model.add(Conv1D(100, 10, activation='relu'))  
model.add(Conv1D(100, 10, activation='relu'))  
model.add(MaxPooling1D(3))  
model.add(Conv1D(160, 10, activation='relu'))  
model.add(Conv1D(160, 10, activation='relu'))  
model.add(GlobalAveragePooling1D())  
model.add(Dense(y_train.shape[1], activation='softmax'))
Conv1D Layers: Extract temporal patterns
MaxPooling: Downsampling for efficiency
GlobalAveragePooling: Feature compression
Softmax Output: Multi-class classification
```

---

## ğŸ¯ Training Strategy  

- **Loss Function**: `categorical_crossentropy` (multi-class classification)  
- **Optimizer**: Adam  
- **K-Fold Cross-Validation**: 5-fold  
- **Training Setup**:  
  - `epochs=50`, `batch_size=100`  
  - **Best accuracy**: **91.2%**  
  - **Average accuracy**: **90.9%**  

---

## ğŸ“ˆ Model Performance  

âœ… **Confusion Matrix**: Visualises model accuracy across all classes  
âœ… **Grad-CAM Heatmaps**: Highlights key areas influencing model decisions  
âœ… **Prediction vs. True Labels**: Evaluates model reliability  

Example Confusion Matrix:

```python
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
```

---
## ğŸ”¥ Results & Key Findings  

ğŸ“Œ **Best Model**:  
- **1D-CNN (Conv1D + MaxPooling + GlobalAvgPooling)**  
- **Achieved 89.2% accuracy on validation set**  

ğŸ“Œ **What Worked Well**:  
- **Cross-validation** improved robustness  
- **Grad-CAM** helped explain AI decisions  
- **Data augmentation** enhanced generalisation  

ğŸ“Œ **Future Improvements**:  
- Implement **self-supervised learning** for better feature extraction  
- Deploy model on **embedded devices** for real-time monitoring  

---

## ğŸ“Œ How to Run  

1ï¸âƒ£ **Install dependencies**  
```bash
pip install -r requirements.txt
```
1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
2ï¸âƒ£ **Train the model**
```bash
python src/train.py
```
3ï¸âƒ£ **Evaluate the model**
```bash
python src/evaluate.py
```
4ï¸âƒ£ **Visualise results**
```bash
python src/visualization.py
```

